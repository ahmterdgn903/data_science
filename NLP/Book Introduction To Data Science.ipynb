{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Introduction To Data Science\n",
    "\n",
    "## Chapter 10: Statistical Natural Language Processing for Sentiment Anlyasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### Some interesting open issues in this topic are as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Identification of sarcasm:\n",
    "Sometimes without knowing the peronality of the person yu don't know whether 'bad' means bad or good\n",
    "\n",
    "##### * Lack of text structure:\n",
    "In twitter data it may contain abbrevation, and there may be a lack capital, poor selling, poor punctuation, and poor grammar all of which make it diffucult to analyze the text.\n",
    "\n",
    "##### * Many possible sentiment categories nad degrees: \n",
    "Positive and negative is a simple analysis, one would like to identify the amount of hate there is inside the opinion, how much happiness, how much sadness etc.\n",
    "\n",
    "##### * Identification of the object of analysis:\n",
    "Many concepts can apear in text, and how to detect the object that the oppinion is positive and the object that the oppinion is negative for is an open issue. For example: If you say 'She won him!', this means a positive sentiment for her and a negative sentiment for him, at the same time.\n",
    "\n",
    "##### * Subjective text:\n",
    "Another open challenge is how to analyze very subjective sentences or paragraphs. Sometime even for humans it is very hard to agree on the sentiment of these higly subjective texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input text data in cell, the main task of data cleaning is to remove those characters considered as noise in the data mining process. For instance, comma or colon characters. Of course, in each particular data mining problem different char- acters can be considered as noise, depending on the final objective of the analysis. In our case, we are going to consider that all punctuation characters should be removed, including other non-conventional symbols. In order to perform the data cleaning pro- cess and posterior text representation and analysis we will use the Natural Language Toolkit (NLTK) library for the examples in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "\"They won’t be very interesting, I’m afraid.\", \"The point of these examples is to _learn how basic text \\\n",
    "cleaning works_ on *very simple* data.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step consists of defining a list with all word-vectors in the text. NLTK makes it easy to convert documents-as-strings into word-vectors, a process called tokenizing. See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', ',', 'I', '’', 'm', 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*very', 'simple*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for each line of text in raw_docs, word_tokenize function will set the list of word-vectors. Now we can search the list for punctuation symbols, for instance, and remove them. There are many ways to perform this step. Let us see one possible solution using the String library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that string.punctuation contains a set of common punctuation sym- bols. This list can be modified according to the symbols you want to remove. Let us see with the next example using the Regular Expressions (RE) package how punctu- ation symbols can be removed. Note that many other possibilities to remove symbols exist, such as directly implementing a loop comparing position by position.\n",
    "In the input cell [6], and without going into the details of RE, re.compile contains a list of “expressions”, the symbols contained in string.punctuation. Then, for each item in tokenized_docs that matches an expression/symbol contained in regex, the part of the item corresponding to the punctuation will be sub- stituted by u” (where u refers to unicode encoding). If the item after substitution cor- responds to u”, it will be not included in the final list. If the new item is different from u”, it means that the item contained text other than punctuation, and thus it is included in the new list without punctuation tokenized_docs_no_punctuation. The\n",
    "results of applying this script are shown in the output cell [7].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', 'I', '’', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "tokenized_docs_no_punctuation = [] \n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token) \n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token) \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print (tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that punctuation symbols are removed, and those words containing a punctuation symbol are kept and marked with an initial u. If the reader wants more details, we recommend to read information about the RE package1 for treating expressions.\n",
    "\n",
    "\n",
    "Another important step in many data mining systems for text analysis consists of stemming and lemmatizing. Morphology is the notion that words have a root form. If you want to get to the basic term meaning of the word, you can try applying a stemmer or lemmatizer. This step is useful to reduce the dictionary size and the posterior high-dimensional and sparse feature spaces. NLTK provides different ways of performing this procedure. In the case of running the porter.stem(word) approach, the output is shown next.\n",
    "\n",
    "https://docs.python.org/2/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['here', 'are', 'some', 'veri', 'simpl', 'basic', 'sentenc'],\n",
       " ['they', 'won', '’', 't', 'be', 'veri', 'interest', 'I', '’', 'm', 'afraid'],\n",
       " ['the',\n",
       "  'point',\n",
       "  'of',\n",
       "  'these',\n",
       "  'exampl',\n",
       "  'is',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'how',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'clean',\n",
       "  'work',\n",
       "  'on',\n",
       "  'veri',\n",
       "  'simpl',\n",
       "  'data']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "#each of the following commands perform stemming on word\n",
    "[[porter.stem(word) for word in sents] for sents in tokenized_docs_no_punctuation]\n",
    "# snowball.stem(word)\n",
    "# wordnet.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['here', 'are', 'some', 'veri', 'simpl', 'basic', 'sentenc'],\n",
       " ['they', 'won', '’', 't', 'be', 'veri', 'interest', 'i', '’', 'm', 'afraid'],\n",
       " ['the',\n",
       "  'point',\n",
       "  'of',\n",
       "  'these',\n",
       "  'exampl',\n",
       "  'is',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'how',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'clean',\n",
       "  'work',\n",
       "  'on',\n",
       "  'veri',\n",
       "  'simpl',\n",
       "  'data']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[snowball.stem(word) for word in sents] for sents in tokenized_docs_no_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentence'],\n",
       " ['They',\n",
       "  'won',\n",
       "  '’',\n",
       "  't',\n",
       "  'be',\n",
       "  'very',\n",
       "  'interesting',\n",
       "  'I',\n",
       "  '’',\n",
       "  'm',\n",
       "  'afraid'],\n",
       " ['The',\n",
       "  'point',\n",
       "  'of',\n",
       "  'these',\n",
       "  'example',\n",
       "  'is',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'how',\n",
       "  'basic',\n",
       "  'text',\n",
       "  'cleaning',\n",
       "  'work',\n",
       "  'on',\n",
       "  'very',\n",
       "  'simple',\n",
       "  'data']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[wordnet.lemmatize(word) for word in sents] for sents in tokenized_docs_no_punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of approaches are very useful in order to reduce the exponential number of combinations of words with the same meaning and match similar texts. Words such as “interest” and “interesting” will be converted into the same word “interest” making the comparison of texts easier, as we will see later.\n",
    "\n",
    "Another very useful data cleaning procedure consists of removing HTML entities and tags. Those may contain words and other symbols that were not removed by applying the previous procedures, but that do not provide useful meaning for text analysis and will introduce noise in our posterior text representation procedure. There are many possibilities for removing these tags. Here we show another example using the same NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e2adcc66ce5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Original text:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Cleaned text:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "test_string =\"<p>While many of the stories tugged at the heartstrings, I never felt manipulated by the authors. (Note: Part of the reason why I don’t like the ’Chicken Soup for the Soul’ series is that I feel that the authors are just dying to make the reader clutch for the box of tissues.)</a>\"\n",
    "print ('Original text:')\n",
    "print (test_string)\n",
    "print ('Cleaned text:', nltk.clean_html(test_string.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that tags such as “<p>” and “</a>” have been removed. The reader is referred to the RE package documentation to learn more about how to use it for data cleaning and HTLM parsing to remove tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we have analyzed different techniques for data cleaning, stem- ming, and lemmatizing, and filtering the text to remove other unnecessary tags for posterior text analysis. In order to analyze sentiment from text, the next step consists of having a representation of the text that has been cleaned. Although different representations of text exist, the most common ones are variants of Bag of Words (BoW) models [1]. The basic idea is to think about word frequencies. If we can define a dictionary of possible different words, the number of different existing words will define the length of a feature space to represent each text. See the toy example in Fig. 10.1. Two different texts represent all the available texts we have in this case. The total number of different words in this dictionary is seven, which will represent the length of the feature vector. Then we can represent each of the two available texts in the form of this feature vector by indicating the number of word frequencies, as shown in the bottom of the figure. The last two rows will represent the feature vector codifying each text in our dictionary.\n",
    "\n",
    "Next, we will see a particular case of bag of words, the Vector Space Model of text: TF–IDF (term frequency–inverse distance frequency). First, we need to count the terms per document, which is the term frequency vector. See a code example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Mireia', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Hector', 1)])\n",
      "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
      "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('football', 1)])\n"
     ]
    }
   ],
   "source": [
    "mydoclist = ['Mireia loves me more than Hector loves me',\n",
    "'Sergio likes me more than Mireia loves me',\n",
    "'He likes basketball more than football'] \n",
    "from collections import Counter\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] += 1 \n",
    "    print (tf.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have introduced the Python object called a Counter. Counters are only in Python 2.7 and higher. They are useful because they allow you to perform this exact kind of function: counting in a loop. A Counter is a dictionary subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements are counted from an iterable or initialized from another mapping (or Counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter() # a new, empty counter \n",
    "c = Counter('gallahad') # a new counter from an iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising a KeyError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(['eggs', 'ham']) \n",
    "c['bacon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us call this a first stab at representing documents quantitatively, just by their word counts (also thinking that we may have previously filtered and cleaned the text using previous approaches). Here we show an example for computing the feature vector based on word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [than, football, more, me, Mireia, He, loves, Sergio, likes, Hector, basketball]\n",
      "The doc is \"Mireia loves me more than Hector loves me\"\n",
      "The tf vector for Document 1 is [1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 0]\n",
      "The doc is \"Sergio likes me more than Mireia loves me\"\n",
      "The tf vector for Document 2 is [1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0]\n",
      "The doc is \"He likes basketball more than football\"\n",
      "The tf vector for Document 3 is [1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "All combined, here is our master document term matrix:\n",
      "[[1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 0], [1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 0], [1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "def build_lexicon(corpus):\n",
    "    # define a set with all possible words included inall the sentences or \"corpus\"\n",
    "    lexicon = set() \n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split ()])\n",
    "    return lexicon\n",
    "def tf(term, document):\n",
    "    return freq(term, document) \n",
    "def freq(term, document):\n",
    "    return document.split().count(term) \n",
    "vocabulary = build_lexicon(mydoclist) \n",
    "doc_term_matrix = []\n",
    "print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']' )\n",
    "for doc in mydoclist:\n",
    "    print ('The doc is \"' + doc + '\"') \n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print ('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string)) \n",
    "    doc_term_matrix.append(tf_vector)\n",
    "print ('All combined, here is our master document term matrix:')\n",
    "print (doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every document is in the same feature space, meaning that we can represent the entire corpus in the same dimensional space. Once we have the data in the same feature space, we can start applying some machine learning methods: learning, classifying, clustering, and so on. But actually, we have a few problems. Words are not all equally informative. If words appear too frequently in a single document, they are going to muck up our analysis. We want to perform some weighting of these term frequency vectors into something a bit more representative. That is, we need to do some vector normalizing. One possibility is to ensure that the L2 norm of each vector is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denom: 12\n",
      "denom: 10\n",
      "denom: 6\n",
      "A regular old document term matrix: \n",
      "[[1 0 1 2 1 0 2 0 0 1 0]\n",
      " [1 0 1 2 1 0 1 1 1 0 0]\n",
      " [1 1 1 0 0 1 0 0 1 0 1]]\n",
      "\n",
      "A document term matrix with row-wise L2 norm:\n",
      "[[0.28867513 0.         0.28867513 0.57735027 0.28867513 0.\n",
      "  0.57735027 0.         0.         0.28867513 0.        ]\n",
      " [0.31622777 0.         0.31622777 0.63245553 0.31622777 0.\n",
      "  0.31622777 0.31622777 0.31622777 0.         0.        ]\n",
      " [0.40824829 0.40824829 0.40824829 0.         0.         0.40824829\n",
      "  0.         0.         0.40824829 0.         0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    print('denom:', denom)\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "print('A regular old document term matrix: ')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "print('\\nA document term matrix with row-wise L2 norm:')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have scaled down the vectors so that each element is between [0, 1]. This will avoid getting a diminishing return on the informative value of a word massively used in a particular document. For that, we need to scale down words that appear too frequently in a document.\n",
    "\n",
    "Finally, we have a final task to perform. Just as not all words are equally valuable within a document, not all words are valuable across all documents. We can try reweighting every word by its inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [than, football, more, me, Mireia, He, loves, Sergio, likes, Hector, basketball]\n",
      "The inverse document frequency vector is [0.000000, 1.098612, 0.000000, 0.405465, 0.405465, 1.098612, 0.405465, 1.098612, 0.405465, 1.098612, 1.098612]\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount += 1\n",
    "    return doccount\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / (float(df)) )\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "print ('Our vocabulary vector is [' + ', '.join(list (vocabulary)) + ']')\n",
    "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a general sense of information values per term in our vocabulary, accounting for their relative frequency across the entire corpus. Note that this is an inverse. To get TF–IDF weighted word-vectors, we have to perform the simple calculation of the term frequencies multiplied by the inverse frequency values.\n",
    "\n",
    "In the next example we convert our IDF vector into a matrix where the diagonal is the IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         1.09861229 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.40546511 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.40546511 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.09861229\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.40546511 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.09861229 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.40546511 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.09861229 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.09861229]]\n"
     ]
    }
   ],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat = np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat, idf_vector)\n",
    "    return idf_mat\n",
    "my_idf_matrix = build_idf_matrix(my_idf_vector)\n",
    "print(my_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we can now multiply every term frequency vector by the inverse document frequency matrix. Then, to make sure we are also accounting for words that appear too frequently within documents, we will normalize each document using the L2 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denom: 2.686566545851071\n",
      "denom: 2.3577626380647403\n",
      "denom: 3.785248836330912\n",
      "{'than', 'football', 'more', 'me', 'Mireia', 'He', 'loves', 'Sergio', 'likes', 'Hector', 'basketball'}\n",
      "[[0.         0.         0.         0.49474872 0.24737436 0.\n",
      "  0.49474872 0.         0.         0.67026363 0.        ]\n",
      " [0.         0.         0.         0.52812101 0.2640605  0.\n",
      "  0.2640605  0.71547492 0.2640605  0.         0.        ]\n",
      " [0.         0.56467328 0.         0.         0.         0.56467328\n",
      "  0.         0.         0.20840411 0.         0.56467328]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf = []\n",
    "#performing tf-idf matrix multiplication\n",
    "for tf_vector in doc_term_matrix: \n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "#normalizing\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "print (vocabulary)\n",
    "# np.matrix() just to make it easier to look at\n",
    "print (np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3.1 Bi-Grams and n-Grams\n",
    "It is sometimes useful to take significant bi-grams into the model based on the BoW. Note that this example can be extended to n-grams. In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. The n-grams are typically collected from a text or speech corpus.\n",
    "\n",
    "A n-gram of size 1 is referred to as a “uni-gram”; size 2 is a “bi-gram” (or, less commonly, a “digram”); size 3 is a “tri-gram”. Larger sizes are sometimes referred to by the value of n, e.g., “four-gram”, “five-gram”, and so on. These n-grams can be introduced within the BoW model just by considering each different n-gram as a new position within the feature vector representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Practical Cases\n",
    "Python packages provide useful tools for analyzing text. The reader is referred to the NLTK and Textblob package2 documentation for further details. Here, we will perform all the previously presented procedures for data cleaning, stemming, and representation and introduce some binary learning schemes to learn the text repre- sentations in the feature space. The binary learning schemes will receive examples for training positive and negative sentiment texts and we will apply them later to unseen examples from a test set.\n",
    "\n",
    "We will apply the whole sentiment analysis process in two examples. The first corresponds to the Large Movie reviews dataset [2]. This is one of the largest public available data sets for sentiment analysis, which includes more than 50,000 texts from movie reviews including the groundtruth annotation related to positive and negative movie reviews. As a proof on concept, for this example we use a subset of the dataset consisting of about 30% of the data.\n",
    "\n",
    "The code reuses part of the previous examples for data cleaning, reads training and testing data from the folders as provided by the authors of the dataset. Then, TF–IDF is computed, which performs all steps mentioned previously for computing feature space, normalization, and feature weights. Note that at the end of the script we perform training and testing based on two different state-of-the-art machine learning approaches: Naive Bayes and Support Vector Machines. It is beyond the scope of this chapter to give details of the methods and parameters. The important point here is that the documents are represented in feature spaces that can be used by different data mining tools.\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import svm\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(text):\n",
    "    # Tokenizing text\n",
    "    text_tokenized = [word_tokenize(doc) for doc in text]\n",
    "    # Removing punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string. punctuation))\n",
    "    tokenized_docs_no_punctuation = [] \n",
    "    for review in text_tokenized:\n",
    "        new_review = []\n",
    "        for token in review:\n",
    "            new_token = regex.sub(u'', token)\n",
    "            if not new_token == u'':\n",
    "                new_review.append(new_token)\n",
    "        tokenized_docs_no_punctuation.append(new_review)\n",
    "    # Stemming and Lemmatizing\n",
    "    porter = PorterStemmer() \n",
    "    preprocessed_docs = []\n",
    "    for doc in tokenized_docs_no_punctuation:\n",
    "        final_doc = ''\n",
    "        for word in doc:\n",
    "            final_doc = final_doc + ' ' + porter. stem(word)\n",
    "        preprocessed_docs.append(final_doc)\n",
    "    return preprocessed_docs\n",
    "\n",
    "# #read your train text data here\n",
    "# textTrain=ReadTrainDataText() \n",
    "# preprocessed_docs=BoW(textTrain) \n",
    "# # for train data # Computing TIDF word space\n",
    "# tfidf_vectorizer = TfidfVectorizer(min_df = 1) \n",
    "# trainData = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "# textTest=ReadTestDataText() #read your test text data here\n",
    "# prepro_docs_test=BoW(textTest) # for test data \n",
    "# testData = tfidf_vectorizer.transform(prepro_docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing on training Naive Bayes\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-24cfc96c0d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and testing on training Naive Bayes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtestData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of mislabeled training points out of a total %d points : %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetTrain\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testData' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training and testing on training Naive Bayes')\n",
    "gnb = GaussianNB()\n",
    "testData.todense()\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(trainData.todense())\n",
    "print(\"Number of mislabeled training points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred) .sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing on train with SVM\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'targetTrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-d2971cd7bdf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and testing on train with SVM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of mislabeled test points out of a total %d points : %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetTrain\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'targetTrain' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training and testing on train with SVM')\n",
    "clf = svm.SVC()\n",
    "clf.fit(trainData.todense(), targetTrain)\n",
    "y_pred = clf.predict(trainData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred).sum()))\n",
    "print('Testing on test with already trained SVM') \n",
    "y_pred = clf.predict(testData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],(targetTest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the machine learning implementations provided by the Scikit- learn module used in this example, NLTK also provides useful learning tools for text learning, which also includes Naive Bayes classifiers. Another related pack- age with similar functionalities is Textblob. The results of running the script are shown next.\n",
    "\n",
    "We can see that the training error of Naive Bayes on the selected data is 129/4313 while in testing it is 2087/6292. Interestingly, the training error using SVM is higher (1288/4313), but it provides a better generalization of the test set than Naive Bayes (1680/6292). Thus it seems that Naive Bayes produces more overfitting of the data (selecting particular features for better learning the training data but producing such high modifications of the feature space for testing that cannot be recovered, just reducing the generalization capability of the technique). However, note that this is a simple execution with standard methods on a subset of the dataset provided. More data, as well as many other aspects, will influence the performance. For instance, we could enrich our dictionary by introducing a list of already studied positive and negative words.3 For further details of the analysis of this dataset, the reader is referred to [2].\n",
    "\n",
    "Finally, let us see another example of sentiment analysis based on tweets. Although there is some work using more tweet data4 here we present a reduced set of tweets which are analyzed as in the previous example of movie reviews. The main code remains the same except for the definition of the initial data.\n",
    "\n",
    "3 Such as those provided in http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "\n",
    "4 http://www.sananalytics.com/lab/twitter-sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "textTrain = ['I love this sandwich.', 'This is an amazing place!', 'I feel very good about these beers.', \n",
    "             'This is my best work.', 'What an awesome view', 'I do not like this restaurant', \n",
    "             'I am tired of this stuff.', 'I can not deal with this', 'He is my sworn enemy!', \n",
    "             'My boss ishorrible.']\n",
    "targetTrain = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "preprocessed_docs=BoW(textTrain) \n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1) \n",
    "trainData = tfidf_vectorizer.fit_transform(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing on test Naive Bayes\n",
      "Number of mislabeled training points out of a total 10 points : 0\n",
      "Number of mislabeled test points out of a total 6 points : 2\n",
      "Training and testing on train with SVM\n",
      "Number of mislabeled test points out of a total 10 points : 0\n",
      "Testing on test with already trained SVM\n",
      "Number of mislabeled test points out of a total 6 points : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "textTest = ['The beer was good.', 'I do not enjoy my job', 'I aint feeling dandy today', 'I feel amazing!', \n",
    "            'Gary is a friend of mine.', 'I can not believe I am doing this.']\n",
    "targetTest = [0, 1, 1, 0, 0, 1] \n",
    "preprocessed_docs=BoW(textTest) \n",
    "testData = tfidf_vectorizer.transform(preprocessed_docs)\n",
    "\n",
    "print('Training and testing on test Naive Bayes') \n",
    "gnb = GaussianNB()\n",
    "testData.todense()\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(trainData.todense())\n",
    "print(\"Number of mislabeled training points out of a total %d points : %d\" % (trainData.shape[0],( targetTrain != y_pred).sum()))\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(testData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],( targetTest != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on train with SVM') \n",
    "clf = svm.SVC()\n",
    "clf.fit(trainData.todense(), targetTrain)\n",
    "y_pred = clf.predict(trainData.todense()) \n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred).sum()))\n",
    "print('Testing on test with already trained SVM') \n",
    "y_pred = clf.predict(testData.todense()) \n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],(targetTest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario both learning strategies achieve the same recognition rates in both training and test sets. Note that similar words are shared between tweets. In practice, with real examples, tweets will include unstructured sentences and abbreviations, making recognition harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.5 Conclusions\n",
    "In this chapter, we have analyzed the problem of binary sentiment analysis of text data: data cleaning to remove irrelevant symbols, punctuation and tags; stemming in order to define the same root for different works with the same meaning in terms of sentiment; defining a dictionary of words (including n-grams); and representing text in terms of a feature space with the length of the dictionary. We have also seen cod- ification in this feature space, based on normalized and weighted term frequencies. We have defined feature vectors that can be used by any machine learning tech- nique in order to perform sentiment analysis (binary classification in the examples shown), and reviewed some useful Python packages, such as NLTK and Textblob, for sentiment analysis.\n",
    "\n",
    "As discussed in the introduction of this chapter, we have only reviewed the senti- ment analysis problem and described common procedures for performing the analysis resulting from a binary classification problem. Several open issues can be addressed in further research, such as the identification of sarcasm, a lack of text structure (as in tweets), many possible sentiment categories and degrees (not only binary but also multiclass, regression, and multilabel problems, among others), identification of the object of analysis, or subjective text, to name a few.\n",
    "\n",
    "The tools described in this chapter can define a basis for dealing with those more challenging problems. One recent example of current state-of-the-art research is the work of [3], where deep learning architectures are used for sentiment analysis. Deep learning strategies are currently a powerful tool in the fields of pattern recognition, machine learning, and computer vision, among others; the main deep learning strate- gies are based on neural network architectures. In the work of [3], a deep learning model builds up a representation of whole sentences based on the sentence struc- ture, and it computes the sentiment based on how words form the meaning of longer phrases. In the methods explained in this chapter, n-grams are the only features that capture those semantics. For further discussion in this field, the reader is referred to [4,5].\n",
    "\n",
    "Acknowledgements This chapter was co-written by Sergio Escalera and Santi Seguí."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
